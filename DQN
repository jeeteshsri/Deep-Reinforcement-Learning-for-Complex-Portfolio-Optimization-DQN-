# Deep Reinforcement Learning for Portfolio Optimization
# Enhanced DQN / Double-DQN 

import numpy as np
import pandas as pd
import gym
from gym import spaces
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import deque
import matplotlib.pyplot as plt

SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)
random.seed(SEED)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def generate_price_data(n_assets=5, n_days=800):
    prices = np.zeros((n_days, n_assets))
    prices[0] = 100

    for t in range(1, n_days):
        returns = np.random.normal(0.0006, 0.01, n_assets)
        prices[t] = prices[t - 1] * np.exp(returns)

    dates = pd.date_range("2020-01-01", periods=n_days)
    return pd.DataFrame(prices, index=dates, columns=[f"Asset_{i}" for i in range(n_assets)])

price_df = generate_price_data()

log_returns = np.log(price_df / price_df.shift(1)).dropna()
ma = price_df.rolling(20).mean().dropna()
volatility = log_returns.rolling(20).std().dropna()

data = pd.concat([log_returns, ma, volatility], axis=1).dropna()

# Normalize state features
data = (data - data.mean()) / (data.std() + 1e-8)

class PortfolioEnv(gym.Env):
    def __init__(self, data, n_assets, transaction_cost=0.001):
        super().__init__()

        self.data = data.values
        self.n_assets = n_assets
        self.tc = transaction_cost
        self.current_step = 0

        self.action_space = spaces.Discrete(3 ** n_assets)
        self.observation_space = spaces.Box(
            low=-5, high=5, shape=(self.data.shape[1],), dtype=np.float32
        )

        self.weights = np.ones(n_assets) / n_assets
        self.prev_weights = self.weights.copy()

    def reset(self):
        self.current_step = 0
        self.weights = np.ones(self.n_assets) / self.n_assets
        self.prev_weights = self.weights.copy()
        return self.data[self.current_step]

    def step(self, action):
        action_vector = self._decode_action(action)

        self.prev_weights = self.weights.copy()
        self.weights += 0.02 * action_vector
        self.weights = np.clip(self.weights, 0, 1)
        self.weights /= np.sum(self.weights)

        asset_returns = self.data[self.current_step][:self.n_assets]
        portfolio_return = np.dot(self.weights, asset_returns)

        turnover = np.sum(np.abs(self.weights - self.prev_weights))
        cost = self.tc * turnover

        reward = portfolio_return - cost
        reward /= (np.std(asset_returns) + 1e-6)  # risk-adjusted

        self.current_step += 1
        done = self.current_step >= len(self.data) - 1

        return self.data[self.current_step], reward, done, {}

    def _decode_action(self, action):
        vec = []
        for _ in range(self.n_assets):
            vec.append((action % 3) - 1)
            action //= 3
        return np.array(vec)

class DQN(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, output_dim)
        )

    def forward(self, x):
        return self.net(x)

class ReplayBuffer:
    def __init__(self, capacity=50000):
        self.buffer = deque(maxlen=capacity)

    def push(self, *transition):
        self.buffer.append(transition)

    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        return map(np.array, zip(*batch))

    def __len__(self):
        return len(self.buffer)

env = PortfolioEnv(data, n_assets=5)

state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n

policy_net = DQN(state_dim, action_dim).to(device)
target_net = DQN(state_dim, action_dim).to(device)
target_net.load_state_dict(policy_net.state_dict())

optimizer = optim.Adam(policy_net.parameters(), lr=5e-4)
buffer = ReplayBuffer()

gamma = 0.99
epsilon = 1.0
epsilon_min = 0.05
epsilon_decay = 0.995
batch_size = 64
episodes = 60
tau = 0.01

reward_log = []

for ep in range(episodes):
    state = env.reset()
    ep_reward = 0

    while True:
        if random.random() < epsilon:
            action = env.action_space.sample()
        else:
            with torch.no_grad():
                action = torch.argmax(
                    policy_net(torch.FloatTensor(state).to(device))
                ).item()

        next_state, reward, done, _ = env.step(action)
        buffer.push(state, action, reward, next_state, done)

        state = next_state
        ep_reward += reward

        if len(buffer) > batch_size:
            states, actions, rewards, next_states, dones = buffer.sample(batch_size)

            states = torch.FloatTensor(states).to(device)
            actions = torch.LongTensor(actions).to(device)
            rewards = torch.FloatTensor(rewards).to(device)
            next_states = torch.FloatTensor(next_states).to(device)
            dones = torch.FloatTensor(dones).to(device)

            q_vals = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()

            next_actions = torch.argmax(policy_net(next_states), dim=1)
            next_q = target_net(next_states).gather(
                1, next_actions.unsqueeze(1)
            ).squeeze()

            target = rewards + gamma * next_q * (1 - dones)

            loss = nn.MSELoss()(q_vals, target.detach())

            optimizer.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)
            optimizer.step()

            for tp, pp in zip(target_net.parameters(), policy_net.parameters()):
                tp.data.copy_(tau * pp.data + (1 - tau) * tp.data)

        if done:
            break

    epsilon = max(epsilon_min, epsilon * epsilon_decay)
    reward_log.append(ep_reward)

    print(f"Episode {ep+1}/{episodes} | Reward: {ep_reward:.4f} | Epsilon: {epsilon:.3f}")

def backtest(env, policy=None):
    state = env.reset()
    total_return = 0

    while True:
        if policy:
            with torch.no_grad():
                action = torch.argmax(
                    policy(torch.FloatTensor(state).to(device))
                ).item()
        else:
            action = env.action_space.sample()

        state, reward, done, _ = env.step(action)
        total_return += reward

        if done:
            break

    return total_return

dqn_return = backtest(env, policy_net)
baseline_return = backtest(env, None)

print("\n===== FINAL RESULTS =====")
print(f"Enhanced DQN Return: {dqn_return:.4f}")
print(f"Equal Weight Baseline Return: {baseline_return:.4f}")

plt.figure(figsize=(10, 4))
plt.plot(reward_log)
plt.title("Enhanced DQN Training Rewards")
plt.xlabel("Episode")
plt.ylabel("Reward")
plt.grid(True)
plt.show()
